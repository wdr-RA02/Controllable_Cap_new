import json, os, math
import torch
import utils
import argparse
from utils.logger import TrainScheduler, get_now_str
from utils import distributed as dist, read_config_file, neat_print_dict
from data import build_dataloader, build_pcap_dataset
from model import ConCapModel, ConCapPretrainedModel, sel_device
from model.eval_metric import ImageCaptionMetric
from model.utils import process_vit_pos_emb
from transformers import BlipProcessor, BlipConfig
from tqdm import tqdm

class ConCapTrainer(object):
    def __init__(self, 
                 model, device, 
                 preprocessor, config, 
                 ext_train_cfg:dict=None, 
                 arg_dist:dict=None):
        
        # an entity of ConCap class
        self.model=model
        self.device=device
        self.preprocessor=preprocessor
        self.config=config
        if isinstance(config, str):
            self.config=read_config_file(config)

        # extract trainer config
        self.train_cfg=self.config["train"]

        if ext_train_cfg is not None:
            # fill in the defaults
            ext_train_cfg.update({k:v for k,v in self.train_cfg.items() \
                                if k not in ext_train_cfg})
            self.train_cfg.update(ext_train_cfg)

        self.last_epoch_filename=self.train_cfg.pop("last_epoch_filename", "last.pth")
        # dist config
        self.__init_dist(arg_dist)
        self.work_dir=self.config["work_dir"]
        os.makedirs(self.work_dir, exist_ok=True)

        # eval
        self.__best_cider=0
        self.__best_result={}


    def __init_dist(self, arg_dist:dict=None):
        if arg_dist is None:
            arg_dist=dist.init_dist()
        self.dist_cfg=arg_dist
        self.dist_training=self.dist_cfg["distributed"]
        self._not_main=dist.get_rank()!=0
        # set new device
        if self.dist_training:
            self.device=self.dist_cfg["gpu"]
            self.num_devices=self.dist_cfg["world_size"]
        else:
            self.num_devices=1
        
    def get_optim_and_scheduler(self, params, 
                                scheduler_gen_fn, sch_kwargs:dict=None,
                                last_epoch:int=-1):
        '''
        - params: model_to_train.parameters()
        - config: config_json["train"] aka train_config in train_model()
        
        note that scheduler is generated by exec ``scheduler_gen_fn(optim, lr_end/lr, last_epoch, *args, **kwargs)``
        
        so ``optim, alpha, last_epoch`` is not required in optim_kwargs
        '''
        scheduler_config=self.train_cfg["scheduler"]
        optim=torch.optim.AdamW(params=params,
                    lr=scheduler_config["lr"], 
                    weight_decay=self.train_cfg["weight_decay"])
        
        # scheduler defaults to cosine decay w/ warm_up
        # TODO: support of other scheduler method

        # alp=lr_end/lr
        alpha=float(scheduler_config["lr_end"]/scheduler_config["lr"])
        assert callable(scheduler_gen_fn), "scheduler_gen_fn is not callable"

        if sch_kwargs is None:
            sch_kwargs={}

        sch_kwargs.update(dict(optim=optim, alpha=alpha, last_epoch=last_epoch))
        scheduler=scheduler_gen_fn(**sch_kwargs)
        # scheduler=utils.step_decay_epoch(optim, alpha, 0.9, self.train_cfg["max_epoch"], last_epoch)
        
        return optim, scheduler

    def train(self,
              slice="",
              log_step:int=0,
              use_tboard:bool=None):
        '''
        for management convenience, we drop the support of external train ds for pretrain
        '''
        model_2_t=dist.DDP(self.model,[self.device]) if self.dist_training else self.model
        train_dataset=build_pcap_dataset(self.config, self.preprocessor, split="train", slice=slice)

        batch=self.train_cfg["batch_size_per_gpu"]
        epoches=self.train_cfg["max_epoch"]
        step_epoch=math.ceil(len(train_dataset)/(batch*self.num_devices))
        # warm up for the first epoch
        sch_kwargs={
            # "decay_rate": 0.85,
            # "step_per_epoch": step_epoch,
            "total_steps": step_epoch*epoches,
            "warmup_steps": step_epoch*1
        }
        optim, scheduler=self.get_optim_and_scheduler(params=model_2_t.parameters(),
                                                      scheduler_gen_fn=utils.cosine_decay_warmup,
                                                      sch_kwargs=sch_kwargs)
        
        optim_dict={"optim": optim,
                    "scheduler": scheduler}

        # for the training step, load super().train to let it play
        train_output=self._train(model_2_t, train_dataset, optim_dict, log_step, use_tboard)
        # TODO: post-train eval logic

        return train_output

    def _train(self, 
              model,
              train_dataset,
              optimizer_dict: dict, 
              log_step:int=0,
              use_tboard:bool=None,
              **kwargs):
        '''
        a bare training function

        input args:
        - model: model to train
        - train_dataset: dataset
        - optimizer_dict: a dict of ``{"optim": optim, "scheduler": scheduler}``
        - log_step: log interval
        - use_tboard: whether to use tensorboard

        must implement functions:
        - ``self._load_data_from_loader()``: tells the trainer how to deal with data from dataloader
        - ``self.evaluate()``: how the trainer should eval the model 
        - ``self._eval_after_epoch()``(Recommended): what the trainer should do after the epoch is done 
            (implemention of ``self.evaluate()`` required)
    
        '''
        # enter training mode
        if isinstance(use_tboard, bool):
            self.train_cfg["log_tensorboard"]=use_tboard
        
        print("\n------Enter training------")
        print("------Training Config:-------")
        neat_print_dict(self.train_cfg)
        # dump train_conf
        with open(os.path.join(self.work_dir, \
                               "conf_{}.json".format(get_now_str("%Y%m%d-%H%M%S"))), "w") as f:
            json.dump(self.config, f, indent=4)

        print("------Training Config End-------")

        batch=self.train_cfg["batch_size_per_gpu"]
        epoches=self.train_cfg["max_epoch"]
        # load train dataset and dataloader
        train_dl=build_dataloader(train_dataset, 
                                  batch, 
                                  self.train_cfg["num_workers"],
                                  dist_training=self.dist_training)
        
        step_epoch=math.ceil(len(train_dataset)/(batch*self.num_devices))

        with TrainScheduler(self.config, self.dist_cfg, log_step) as logger:
            # 1. before_train_begin
            start_time=get_now_str("%y%m%d-%H%M%S")
            logger.before_train_begin(epoches, step_epoch*epoches)
            # if no other "eval_comment", then set to start_time
            kwargs.setdefault("eval_comment", start_time)
            # real training process
            output=self._core_train(model, train_dl, optimizer_dict, 
                                    epoches, logger, **kwargs)
            
            logger.after_train_end()
        
        if self.__best_result is not None:
            print("Best evaluation result: ")
            # print("Epoch: {}".format(self.__best_result["epoch"]))
            print(self.__best_result)
        
        return output
    

    def _core_train(self, model, train_dl, optimizer_dict:dict, 
                    epoches, logger:TrainScheduler, 
                    eval_comment, **after_epoch_kwargs):
        '''
        the func where magic happens for one epoch

        - after_epoch_kwargs: items you hope to insert to _after_epoch()

        (model_output and optimizer_dict is included by default)
        '''
        in_key=set(map(lambda x: (x in optimizer_dict), ["optim", "scheduler"]))
        assert in_key == {True},\
               "Optimizer or scheduler is not given, got {}".format(optimizer_dict.keys())
        
        optim=optimizer_dict["optim"]
        scheduler=optimizer_dict["scheduler"]

        for epoch in range(1,epoches+1):
            # 2. before_epoch_begin
            logger.before_epoch_begin(epoch)
            model.train()
            if self.dist_training:
                # set shuffle epoch w
                train_dl.sampler.set_epoch(epoch)
            
            for iter, item in enumerate(train_dl):
                iter_item=self._load_data_from_loader(item)
                # forward prop
                output=model(**iter_item)
                loss=self._loss_from_output(output)
                loss.backward()

                # perform a back_prop
                optim.step()
                optim.zero_grad()
                scheduler.step()
                current_lr=optim.param_groups[0]["lr"]
                # log any detailed 
                logger.after_iter_end(epoch, iter, loss.item(), current_lr)
            
            logger.after_epoch_end(epoch)
            # insert output of model into after_epoch_kwargs
            after_epoch_kwargs.update({"model_output": output, **optimizer_dict})
            self._after_epoch(epoch, model, eval_comment=eval_comment, **after_epoch_kwargs)
    
    def _loss_from_output(self, output:dict):
        losses=[output[i] for i in output.keys() if i.startswith("loss_")]
        
        loss=torch.zeros([], device=losses[0].device)
        for item in losses:
            loss+=item
        
        return loss

    def _load_data_from_loader(self, item, *args, **kwargs):
        return dict(
            **{k:v.to(self.device) for k,v in item.items()},
            labels=item["input_ids"].clone().to(self.device)
        )
    
    def _save_last_epoch(self, model):
        if self._not_main:
            return
        
        model_no_ddp=model.module if self.dist_training else model
        last_model_filename=os.path.join(self.work_dir, self.last_epoch_filename)
        print("Saving current model to {}".format(last_model_filename))
        torch.save(model_no_ddp.state_dict(), last_model_filename)
        print("Save done")
            

    def save_trained(self, tgt_dir=None):
        if self._not_main:
            # this process only runs in main machine
            return
        
        if tgt_dir is None:
            tgt_dir = os.path.join(self.config["work_dir"], "outputs")
        
        assert not os.path.isfile(tgt_dir), "Target path {} is an existent file".format(tgt_dir)
        os.makedirs(tgt_dir, exist_ok=True)
        # save model and vocab
        self.preprocessor.save_pretrained(tgt_dir)

        state_dict=self.model.state_dict()
        if not self.model.prefix_vit_():
            state_dict={k:v for k,v in state_dict.items() if not k.startswith("vision_prefix.")}
        
        if not self.model.prefix_decoder_():
            state_dict={k:v for k,v in state_dict.items() if not k.startswith("decoder_prefix.")}

        self.model.save_pretrained(tgt_dir, state_dict=state_dict)

        print("Model and preprocessor saved to {}".format(tgt_dir))

    @torch.no_grad()
    def evaluate(self, model_no_ddp=None):
        print("\n------Enter evaluation------")
        model=self.model if model_no_ddp is None else model_no_ddp
        model.eval()
        batch=self.train_cfg["batch_size_per_gpu"]
        num_workers=self.train_cfg["num_workers"]

        eos_token=self.preprocessor.tokenizer.sep_token
        pad_token=self.preprocessor.tokenizer.pad_token
        evaluator=ImageCaptionMetric(mul_100=True, eos_token=eos_token, pad_token=pad_token)

        test_ds=build_pcap_dataset(self.config, self.preprocessor, self.device, "test")
        steps=math.ceil(len(test_ds)/batch)
        test_dl=build_dataloader(test_ds, batch=batch, num_workers=num_workers, dist_training=False)

        for i, test_item in tqdm(enumerate(test_dl), \
                                 total=steps, desc="Processing test set"):
            pixels=test_item.pop("pixel_values").to(self.device)
            prefix_ids=test_item.pop("prefix_ids").to(self.device)

            outputs=model.generate(pixel_values=pixels, 
                                   prefix_ids=prefix_ids,
                                   num_beams=3,
                                   repetition_penalty=1.2,
                                   max_length=30)
            outputs=self.preprocessor.batch_decode(outputs, skip_special_tokens=True)
            
            evaluator.add(outputs, test_item, batch_id="batch%d" % i, from_dl=True)
        
        result=evaluator.evaluate()
        print("\n------Finished evaluation------")
        # round the results
        result={k:round(v, 3) for k,v in result.items()}
        return result

    def _after_epoch(self, epoch, model, eval_comment:str, *args, **kwargs):
        if self._not_main:
            return
        
        model_no_ddp=model.module if self.dist_training else model
        eval_result=self.evaluate(model_no_ddp)
        print(eval_result)
        # save best model based on CIDEr score
        eval_out_file=os.path.join(self.config["work_dir"], 
                                   "evalresult_{}.json".format(eval_comment))
        new_file=not os.path.exists(eval_out_file)

        # save best model after each epoch
        # save first to avoid nasty json-reading bugs....
        result_this_epoch={
            "eval_time": get_now_str("%y%m%d-%H%M%S"),
            "epoch": epoch,
            "result": eval_result
        }

        if eval_result["CIDEr"]>=self.__best_cider:
            self.__best_cider=eval_result["CIDEr"]
            self.__best_result=result_this_epoch.copy()
            
            best_model_filename=os.path.join(self.work_dir, "best.pth")
            print("Saving current best model to {}".format(best_model_filename))
            torch.save(model_no_ddp.state_dict(), best_model_filename)
            print("Save done")
        
        if new_file:
            f=open(eval_out_file, "w+")
            past_result=[]
        else:
            f=open(eval_out_file, "r+")
            past_result=json.load(f)

        past_result.append(result_this_epoch)
        # move the seek to the head to write
        f.seek(0,0)
        json.dump(past_result, f, indent=4)
        f.close()

        # else we just save every last epoch as last_pretrain.pth        
        self._save_last_epoch(model)


def load_model(config, pretrain_path:str, device):
    if isinstance(config, str):
        config=read_config_file(config)

    print("-------initializing base model from {} --------".format(pretrain_path))
    img_size=config["vision_model"].get("image_size", 384)
    print("Image size:{}\n".format(img_size))

    # pass size argument 
    preprocessor=BlipProcessor.from_pretrained(pretrain_path, size=img_size)
    model_conf:BlipConfig = BlipConfig.from_pretrained(pretrain_path)
    model_conf.vision_config.image_size=img_size

    model, load_info=ConCapModel.from_pretrained(pretrain_path, 
                                                 pfx_config_dict=config["text_model"],
                                                 config=model_conf,
                                                 ignore_mismatched_sizes=True,
                                                 output_loading_info=True)
    
    # interpolate in case 
    model=process_vit_pos_emb(pretrain_path, model)
    # freeze vit, train dec only
    model.text_decoder.requires_grad_(False)
    model.vision_model.requires_grad_(False)
    model.vision_prefix.requires_grad_(False)
    # model.vis_abstr.requires_grad_(False)

    model.to(device)

    print("-------done initializing model-------")

    return model, preprocessor, load_info


if __name__ == "__main__":
    parser=argparse.ArgumentParser()
    parser.add_argument("--conf", type=str, required=True)
    parser.add_argument("-b", "--batch_size", type=int, default=4)
    parser.add_argument("-w", "--num_workers", type=int, default=1)
    parser.add_argument("-e", "--epoches", type=int, default=3)
    parser.add_argument("-d", "--weight_decay", type=float, default=0.05)
    parser.add_argument("--lr", type=float, default=1e-5)
    parser.add_argument("--lr_end", type=float, default=2.5e-7)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--ckpt_from", type=str)
    args=parser.parse_args()

    dist_config=dist.init_dist()
    dist.init_seed(args.seed)
    device=dist_config.get("gpu", sel_device())

    config=read_config_file(args.conf)
    pt_path=args.ckpt_from
    if pt_path is None:
        pt_path = "./work_dir/pretrain/outputs/"

    train_config={
        "batch_size_per_gpu": args.batch_size,
        "num_workers": args.num_workers,
        "scheduler":{
            "lr": args.lr,
            "lr_end": args.lr_end,
        },
        "weight_decay": args.weight_decay,
        "max_epoch": args.epoches
    }

    model, processor, load_info=load_model(config, pt_path, device)
    # freeze vit and vit_prefix, since we don't need to train them
    # model.vision_model.requires_grad_(False)
    # model.vision_prefix.requires_grad_(False)

    trainer=ConCapTrainer(model, device, processor, config, 
                          train_config, dist_config)
    
    print(model.get_pfx_status())
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(name)
    
    results=trainer.train(log_step=50)
    # save model
    trainer.save_trained()