import json, os, math
import torch
import utils
import argparse
from utils.logger import TrainScheduler, get_now_str
from utils import distributed as dist, read_config_file, neat_print_dict
from data import build_dataloader, build_pcap_dataset
from model import ConCapPretrainedModel, sel_device
from model.eval_metric import ImageCaptionMetric
from model.utils import process_vit_pos_emb
from transformers import BlipProcessor, BlipConfig

class ConCapTrainerBase(object):
    def __init__(self, 
                 model, device, 
                 preprocessor, config, 
                 ext_train_cfg:dict=None, 
                 arg_dist:dict=None):
        
        # an entity of ConCap class
        self.model=model
        self.device=device
        self.preprocessor=preprocessor
        self.config=config
        if isinstance(config, str):
            self.config=read_config_file(config)

        # extract trainer config
        self.train_cfg=self.config["train"]

        if ext_train_cfg is not None:
            # fill in the defaults
            ext_train_cfg.update({k:v for k,v in self.train_cfg.items() \
                                if k not in ext_train_cfg})
            self.train_cfg.update(ext_train_cfg)

        self.last_epoch_filename=self.train_cfg.pop("last_epoch_filename", "last.pth")
        # dist config
        self.__init_dist(arg_dist)
        self.work_dir=self.config["work_dir"]
        os.makedirs(self.work_dir, exist_ok=True)

    def __init_dist(self, arg_dist:dict=None):
        if arg_dist is None:
            arg_dist=dist.init_dist()
        self.dist_cfg=arg_dist
        self.dist_training=self.dist_cfg["distributed"]
        self._not_main=dist.get_rank()!=0
        # set new device
        if self.dist_training:
            self.device=self.dist_cfg["gpu"]
            self.num_devices=self.dist_cfg["world_size"]
        else:
            self.num_devices=1
        
    def get_optim_and_scheduler(self, params, 
                                scheduler_gen_fn, sch_kwargs:dict=None,
                                last_epoch:int=-1):
        '''
        - params: model_to_train.parameters()
        - config: config_json["train"] aka train_config in train_model()
        
        note that scheduler is generated by exec ``scheduler_gen_fn(optim, lr_end/lr, last_epoch, *args, **kwargs)``
        
        so ``optim, alpha, last_epoch`` is not required in optim_kwargs
        '''
        scheduler_config=self.train_cfg["scheduler"]
        optim=torch.optim.AdamW(params=params,
                    lr=scheduler_config["lr"], 
                    weight_decay=self.train_cfg["weight_decay"])
        
        # scheduler defaults to cosine decay w/ warm_up
        # TODO: support of other scheduler method

        # alp=lr_end/lr
        alpha=float(scheduler_config["lr_end"]/scheduler_config["lr"])
        assert callable(scheduler_gen_fn), "scheduler_gen_fn is not callable"

        if sch_kwargs is None:
            sch_kwargs={}

        sch_kwargs.update(dict(optim=optim, alpha=alpha, last_epoch=last_epoch))
        scheduler=scheduler_gen_fn(**sch_kwargs)
        # scheduler=utils.step_decay_epoch(optim, alpha, 0.9, self.train_cfg["max_epoch"], last_epoch)
        
        return optim, scheduler

    def train(self,
              slice="",
              log_step:int=0,
              use_tboard:bool=None):
        '''
        for management convenience, we drop the support of external train ds for pretrain
        '''
        model_2_t=dist.DDP(self.model,[self.device]) if self.dist_training else self.model
        train_dataset=build_pcap_dataset(self.config, self.preprocessor, split="pretrain", slice=slice)

        batch=self.train_cfg["batch_size_per_gpu"]
        epoches=self.train_cfg["max_epoch"]
        step_epoch=math.ceil(len(train_dataset)/(batch*self.num_devices))
        # warm up for the first epoch
        sch_kwargs={
            # "decay_rate": 0.85,
            # "step_per_epoch": step_epoch,
            "total_steps": step_epoch*epoches,
            "warmup_steps": step_epoch*1
        }
        optim, scheduler=self.get_optim_and_scheduler(params=model_2_t.parameters(),
                                                      scheduler_gen_fn=utils.cosine_decay_warmup,
                                                      sch_kwargs=sch_kwargs)
        
        optim_dict={"optim": optim,
                    "scheduler": scheduler}

        # for the training step, load super().train to let it play
        train_output=self._train(model_2_t, train_dataset, optim_dict, log_step, use_tboard)
        # TODO: post-train eval logic

        return train_output

    def _after_epoch(self, epoch, model, *args, **kwargs):
        if self._not_main:
            return
        
        # if save every is defined
        save_interval=int(self.train_cfg.get("save_interval", 0))
        last_epoch=self.train_cfg["max_epoch"]

        # we include the last epoch into saving range
        save_this=(save_interval > 0) and (epoch % save_interval == 0 or epoch==last_epoch)
        if not save_this:
            return
        
        self.last_epoch_filename="pretrain_epoch{}.pth".format(epoch)
        # else we just save every last epoch as last_pretrain.pth        
        self._save_last_epoch(model)

    def _train(self, 
              model,
              train_dataset,
              optimizer_dict: dict, 
              log_step:int=0,
              use_tboard:bool=None,
              **kwargs):
        '''
        a bare training function

        input args:
        - model: model to train
        - train_dataset: dataset
        - optimizer_dict: a dict of ``{"optim": optim, "scheduler": scheduler}``
        - log_step: log interval
        - use_tboard: whether to use tensorboard

        must implement functions:
        - ``self._load_data_from_loader()``: tells the trainer how to deal with data from dataloader
        - ``self.evaluate()``: how the trainer should eval the model 
        - ``self._eval_after_epoch()``(Recommended): what the trainer should do after the epoch is done 
            (implemention of ``self.evaluate()`` required)
    
        '''
        # enter training mode
        if isinstance(use_tboard, bool):
            self.train_cfg["log_tensorboard"]=use_tboard
        
        print("\n------Enter training------")
        print("------Training Config:-------")
        neat_print_dict(self.train_cfg)
        # dump train_conf
        with open(os.path.join(self.work_dir, \
                               "conf_{}.json".format(get_now_str("%Y%m%d-%H%M%S"))), "w") as f:
            json.dump(self.config, f, indent=4)

        print("------Training Config End-------")

        batch=self.train_cfg["batch_size_per_gpu"]
        epoches=self.train_cfg["max_epoch"]
        # load train dataset and dataloader
        train_dl=build_dataloader(train_dataset, 
                                  batch, 
                                  self.train_cfg["num_workers"],
                                  dist_training=self.dist_training)
        
        step_epoch=math.ceil(len(train_dataset)/(batch*self.num_devices))

        with TrainScheduler(self.config, self.dist_cfg, log_step) as logger:
            # 1. before_train_begin
            start_time=get_now_str("%y%m%d-%H%M%S")
            logger.before_train_begin(epoches, step_epoch*epoches)
            # if no other "eval_comment", then set to start_time
            kwargs.setdefault("eval_comment", start_time)
            # real training process
            output=self._core_train(model, train_dl, optimizer_dict, 
                                    epoches, logger, **kwargs)
            
            logger.after_train_end()
        
        return output

    def _core_train(self, model, train_dl, optimizer_dict:dict, 
                    epoches, logger:TrainScheduler, 
                    eval_comment, **after_epoch_kwargs):
        '''
        the func where magic happens for one epoch

        - after_epoch_kwargs: items you hope to insert to _after_epoch()

        (model_output and optimizer_dict is included by default)
        '''
        in_key=set(map(lambda x: (x in optimizer_dict), ["optim", "scheduler"]))
        assert in_key == {True},\
               "Optimizer or scheduler is not given, got {}".format(optimizer_dict.keys())
        
        optim=optimizer_dict["optim"]
        scheduler=optimizer_dict["scheduler"]

        for epoch in range(1,epoches+1):
            # 2. before_epoch_begin
            logger.before_epoch_begin(epoch)
            model.train()
            if self.dist_training:
                # set shuffle epoch w
                train_dl.sampler.set_epoch(epoch)
            
            for iter, item in enumerate(train_dl):
                iter_item=self._load_data_from_loader(item)
                # forward prop
                output=model(**iter_item)
                loss=self._loss_from_output(output)
                loss.backward()

                # perform a back_prop
                optim.step()
                optim.zero_grad()
                scheduler.step()
                current_lr=optim.param_groups[0]["lr"]
                # log any detailed 
                logger.update_logs(**{x: round(output[x].item(), 6) \
                                    for x in output if x.startswith("loss_")})
                logger.after_iter_end(epoch, iter, loss.item(), current_lr)
            
            logger.after_epoch_end(epoch)
            # insert output of model into after_epoch_kwargs
            after_epoch_kwargs.update({"model_output": output, **optimizer_dict})
            self._after_epoch(epoch, model, eval_comment=eval_comment, **after_epoch_kwargs)
    
    def _loss_from_output(self, output:dict):
        losses=[output[i] for i in output.keys() if i.startswith("loss_")]
        
        loss=torch.zeros([], device=losses[0].device)
        for item in losses:
            loss+=item
        
        return loss
    
    def _load_data_from_loader(self, item, *args, **kwargs):
        pixel_values=item.pop("pixel_values").to(self.device)
        bsz=pixel_values.shape[0]
        merges={}
        # add label first

        for merge_keys in ["input_ids", "attention_mask", "prefix_ids"]:
            gnd_truth=item.pop(merge_keys)
            fake=item.pop("neg_{}".format(merge_keys))
            merges[merge_keys]=torch.cat([gnd_truth, fake], dim=0).to(self.device)

        labels=merges["input_ids"].clone()[0:bsz, :]

        return dict(pixel_values=pixel_values, labels=labels, **merges)
    
    def _save_last_epoch(self, model):
        if self._not_main:
            return
        
        model_no_ddp=model.module if self.dist_training else model
        last_model_filename=os.path.join(self.work_dir, self.last_epoch_filename)
        print("Saving current model to {}".format(last_model_filename))
        torch.save(model_no_ddp.state_dict(), last_model_filename)
        print("Save done")
            

    def save_trained(self, tgt_dir=None):
        if self._not_main:
            # this process only runs in main machine
            return
        
        if tgt_dir is None:
            tgt_dir = os.path.join(self.config["work_dir"], "outputs")
        
        assert not os.path.isfile(tgt_dir), "Target path {} is an existent file".format(tgt_dir)
        os.makedirs(tgt_dir, exist_ok=True)
        # save model and vocab
        self.preprocessor.save_pretrained(tgt_dir)

        state_dict=self.model.state_dict()
        if not self.model.prefix_vit_():
            state_dict={k:v for k,v in state_dict.items() if not k.startswith("vision_prefix.")}
        
        if not self.model.prefix_decoder_():
            state_dict={k:v for k,v in state_dict.items() if not k.startswith("decoder_prefix.")}

        self.model.save_pretrained(tgt_dir, state_dict=state_dict)

        print("Model and preprocessor saved to {}".format(tgt_dir))


def load_model(config, pretrain_path:str, device):
    if isinstance(config, str):
        config=read_config_file(config)

    print("-------initializing base model from {} --------".format(pretrain_path))
    img_size=config["vision_model"].get("image_size", 384)
    print("Image size:{}\n".format(img_size))

    # pass size argument 
    preprocessor=BlipProcessor.from_pretrained(pretrain_path, size=img_size)
    model_conf:BlipConfig = BlipConfig.from_pretrained(pretrain_path)
    model_conf.vision_config.image_size=img_size

    print("-------loading ckpt for pretrain-------")
    model, load_info=ConCapPretrainedModel.from_pretrained(pretrain_path, 
                                                pfx_config_dict=config["text_model"],
                                                config=model_conf,
                                                ignore_mismatched_sizes=True,
                                                output_loading_info=True)
    # freeze vit
    model.text_encoder.requires_grad_(False)
    model.text_decoder.requires_grad_(False)
    model.vision_proj.requires_grad_(False)
    model.text_proj.requires_grad_(False)
    model.vision_model.requires_grad_(False)
    model.vis_abstr.requires_grad_(True)

    # interpolate in case 
    model=process_vit_pos_emb(pretrain_path, model)
    model.to(device)

    print("-------done initializing model-------")

    return model, preprocessor, load_info


if __name__ == "__main__":
    parser=argparse.ArgumentParser()
    parser.add_argument("--conf", type=str, required=True)
    parser.add_argument("-b", "--batch_size", type=int, default=4)
    parser.add_argument("-w", "--num_workers", type=int, default=1)
    parser.add_argument("-e", "--epoches", type=int, default=3)
    parser.add_argument("-d", "--weight_decay", type=float, default=0.05)
    parser.add_argument("--lr", type=float, default=1e-5)
    parser.add_argument("--lr_end", type=float, default=2.5e-7)
    parser.add_argument("--seed", type=int, default=42)
    args=parser.parse_args()

    dist_config=dist.init_dist()
    dist.init_seed(args.seed)
    device=dist_config.get("gpu", sel_device())

    config=read_config_file(args.conf)
    pt_path="./work_dir/pt_source/"

    train_config={
        "batch_size_per_gpu": args.batch_size,
        "num_workers": args.num_workers,
        "scheduler":{
            "lr": args.lr,
            "lr_end": args.lr_end,
        },
        "weight_decay": args.weight_decay,
        "max_epoch": args.epoches,
        "save_interval": 2
    }

    model, processor, load_info=load_model(config, pt_path, device)
    trainer=ConCapTrainerBase(model, device, processor, config, 
                              train_config, dist_config)
    
    print(model.get_pfx_status())
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(name)
    
    results=trainer.train(log_step=50)
    # save model
    trainer.save_trained()