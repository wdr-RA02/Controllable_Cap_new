{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa38c388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "import torch\n",
    "\n",
    "device=[\"cpu\", \"cuda\"][torch.cuda.is_available()]\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "467ee7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path as osp\n",
    "\n",
    "with open(\"./conf/pretrain_pcap.json\") as f:\n",
    "    config=json.load(f)\n",
    "\n",
    "pt=\"./work_dir/pt_source/\"\n",
    "mod=\"Salesforce/blip-image-captioning-base\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cf67365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------initializing base model from ./work_dir/pt_source/ --------\n",
      "Image size:224\n",
      "\n",
      "-------loading ckpt for pretrain-------\n"
     ]
    }
   ],
   "source": [
    "import evaluate_model\n",
    "evaluate_model.tqdm_module(True)\n",
    "from pretrain import load_model\n",
    "\n",
    "model, processor, load_info=load_model(config, pt, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58ebd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ViT_prefix': True, 'Text_dec_prefix': False}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_pfx_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00674b8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "load_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef2c9216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cached result\n",
    "def load_cached_result(json_filename:str):\n",
    "    if osp.exists(json_filename):\n",
    "        f=open(json_filename, \"r+\")\n",
    "        cached_results=json.load(f)\n",
    "        f.close()\n",
    "    else:\n",
    "        cached_results=[]\n",
    "    \n",
    "    return cached_results\n",
    "\n",
    "def dump_result(results, json_fname:str):\n",
    "    with open(json_fname, \"w+\") as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09b1a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "\n",
    "# return all ckpt files from \n",
    "def find_ckpt_files(dir, pattern):\n",
    "    files=os.listdir(dir)\n",
    "    ckpts=list(filter(lambda x: re.match(pattern, x) is not None, files))\n",
    "    # extract epoch numbers from valid ckpt list\n",
    "    epoches=list(map(lambda x:int(re.findall(pattern, x)[0]), ckpts))\n",
    "    # concat address\n",
    "    ckpts=list(map(lambda x:os.path.join(dir, x), ckpts))\n",
    "\n",
    "    zipped_ckpts=list(zip(epoches,ckpts))\n",
    "\n",
    "    return sorted(zipped_ckpts, key=lambda x:x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29e91c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the results in pd\n",
    "import pandas as pd\n",
    "\n",
    "def dump_dataframe(results, index=\"epoch\"):\n",
    "    df=pd.read_json(json.dumps(results)).set_index(index)\n",
    "    df=df.sort_index()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff854eb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## test_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3f5d78d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import BlipConfig, BlipProcessor\n",
    "from model import ConCapPretrainedModel\n",
    "\n",
    "pt=\"./work_dir/pt_source/\"\n",
    "\n",
    "model_conf=BlipConfig.from_pretrained(pt)\n",
    "# test_mo=BlipModel.from_pretrained(mod)\n",
    "test_itc=model\n",
    "preprocessor=BlipProcessor.from_pretrained(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307f9094",
   "metadata": {},
   "source": [
    "### for test new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a988ddda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./work_dir/pt_source_14m/ were not used when initializing ConCapModel: ['text_encoder.encoder.layer.8.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.crossattention.self.query.bias', 'text_encoder.encoder.layer.7.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.crossattention.self.key.bias', 'text_encoder.encoder.layer.11.output.dense.weight', 'text_encoder.encoder.layer.1.attention.output.dense.bias', 'text_encoder.encoder.layer.1.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.output.dense.bias', 'text_encoder.encoder.layer.3.attention.self.key.bias', 'text_encoder.encoder.layer.9.output.dense.weight', 'text_encoder.encoder.layer.1.attention.self.value.bias', 'text_encoder.encoder.layer.4.attention.self.value.bias', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.crossattention.self.query.weight', 'text_encoder.encoder.layer.11.crossattention.self.query.weight', 'text_encoder.encoder.layer.5.attention.output.dense.weight', 'text_encoder.encoder.layer.10.attention.self.key.weight', 'text_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.intermediate.dense.bias', 'text_encoder.encoder.layer.5.output.dense.bias', 'text_encoder.encoder.layer.2.crossattention.self.value.bias', 'text_encoder.encoder.layer.11.crossattention.output.dense.bias', 'text_encoder.encoder.layer.0.intermediate.dense.weight', 'text_encoder.encoder.layer.10.attention.self.query.bias', 'text_encoder.encoder.layer.4.attention.self.value.weight', 'text_encoder.encoder.layer.10.intermediate.dense.weight', 'text_encoder.encoder.layer.5.crossattention.output.dense.weight', 'text_encoder.encoder.layer.4.output.dense.bias', 'text_encoder.encoder.layer.5.attention.self.value.bias', 'text_encoder.encoder.layer.3.crossattention.self.value.weight', 'text_encoder.encoder.layer.8.crossattention.self.query.bias', 'text_encoder.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.attention.self.query.bias', 'text_encoder.encoder.layer.7.crossattention.self.key.bias', 'text_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.crossattention.self.key.bias', 'text_encoder.embeddings.position_ids', 'text_encoder.encoder.layer.0.crossattention.self.key.bias', 'text_encoder.encoder.layer.3.attention.self.value.weight', 'text_encoder.encoder.layer.5.crossattention.self.value.bias', 'text_encoder.encoder.layer.0.crossattention.self.query.bias', 'text_encoder.encoder.layer.7.crossattention.self.value.bias', 'text_encoder.encoder.layer.1.crossattention.self.query.bias', 'text_encoder.encoder.layer.1.crossattention.output.dense.weight', 'text_encoder.encoder.layer.10.crossattention.output.dense.weight', 'text_encoder.encoder.layer.4.intermediate.dense.weight', 'text_encoder.encoder.layer.6.attention.self.query.bias', 'text_encoder.encoder.layer.6.crossattention.output.dense.bias', 'text_encoder.encoder.layer.10.crossattention.self.value.bias', 'text_encoder.encoder.layer.3.crossattention.self.key.weight', 'text_encoder.encoder.layer.5.attention.self.query.bias', 'text_encoder.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.intermediate.dense.bias', 'text_encoder.encoder.layer.6.attention.self.query.weight', 'text_encoder.encoder.layer.11.crossattention.self.value.weight', 'text_encoder.encoder.layer.3.crossattention.self.value.bias', 'text_encoder.encoder.layer.3.intermediate.dense.bias', 'text_encoder.encoder.layer.5.output.dense.weight', 'text_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.crossattention.self.query.weight', 'text_encoder.encoder.layer.7.output.dense.weight', 'text_encoder.encoder.layer.9.crossattention.self.value.weight', 'text_encoder.encoder.layer.7.attention.self.key.bias', 'text_encoder.encoder.layer.0.crossattention.self.value.bias', 'text_encoder.encoder.layer.10.attention.self.value.weight', 'text_encoder.encoder.layer.7.crossattention.output.dense.weight', 'text_proj.weight', 'text_encoder.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.attention.self.query.weight', 'text_encoder.encoder.layer.0.crossattention.output.dense.weight', 'text_encoder.encoder.layer.1.crossattention.self.value.weight', 'text_encoder.encoder.layer.5.intermediate.dense.weight', 'text_encoder.encoder.layer.9.crossattention.output.dense.weight', 'text_encoder.encoder.layer.3.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.attention.self.key.weight', 'text_encoder.encoder.layer.10.crossattention.output.dense.bias', 'text_encoder.encoder.layer.7.attention.self.query.weight', 'text_encoder.encoder.layer.7.attention.output.dense.weight', 'text_encoder.encoder.layer.3.attention.self.query.bias', 'text_encoder.encoder.layer.11.crossattention.self.query.bias', 'text_encoder.encoder.layer.6.attention.self.key.bias', 'text_encoder.encoder.layer.0.attention.self.value.weight', 'text_encoder.encoder.layer.10.crossattention.self.key.bias', 'text_encoder.encoder.layer.8.attention.self.value.weight', 'text_encoder.encoder.layer.9.intermediate.dense.weight', 'itm_head.bias', 'text_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.attention.self.key.bias', 'text_encoder.encoder.layer.5.intermediate.dense.bias', 'text_encoder.encoder.layer.8.crossattention.self.key.weight', 'text_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.attention.self.key.weight', 'text_encoder.encoder.layer.10.crossattention.self.value.weight', 'text_encoder.encoder.layer.6.intermediate.dense.bias', 'text_encoder.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.crossattention.self.query.bias', 'text_encoder.encoder.layer.4.attention.self.query.weight', 'text_encoder.encoder.layer.8.attention.self.key.weight', 'text_encoder.encoder.layer.0.attention.self.key.weight', 'text_encoder.encoder.layer.5.attention.self.key.bias', 'text_encoder.encoder.layer.0.crossattention.self.query.weight', 'text_encoder.encoder.layer.2.attention.output.dense.bias', 'text_encoder.encoder.layer.7.crossattention.self.key.weight', 'text_encoder.encoder.layer.0.attention.self.query.weight', 'text_encoder.encoder.layer.1.intermediate.dense.bias', 'text_encoder.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.intermediate.dense.bias', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.attention.self.key.weight', 'text_encoder.encoder.layer.7.crossattention.self.value.weight', 'text_encoder.encoder.layer.1.output.dense.bias', 'text_encoder.encoder.layer.3.crossattention.self.query.weight', 'text_encoder.encoder.layer.2.attention.self.query.weight', 'text_encoder.encoder.layer.5.crossattention.self.query.weight', 'text_encoder.encoder.layer.11.intermediate.dense.bias', 'text_encoder.encoder.layer.6.output.dense.bias', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.attention.output.dense.weight', 'text_encoder.encoder.layer.3.attention.output.dense.bias', 'text_encoder.encoder.layer.8.attention.self.value.bias', 'text_encoder.encoder.layer.10.intermediate.dense.bias', 'text_encoder.encoder.layer.6.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.crossattention.output.dense.weight', 'text_encoder.encoder.layer.3.attention.output.dense.weight', 'text_encoder.encoder.layer.2.output.dense.bias', 'text_encoder.encoder.layer.11.output.LayerNorm.weight', 'text_encoder.embeddings.LayerNorm.weight', 'text_encoder.encoder.layer.6.crossattention.self.key.weight', 'text_encoder.encoder.layer.4.attention.self.query.bias', 'text_encoder.encoder.layer.8.output.dense.bias', 'text_encoder.encoder.layer.6.attention.self.value.weight', 'text_encoder.encoder.layer.0.attention.output.dense.bias', 'text_encoder.embeddings.position_embeddings.weight', 'text_encoder.encoder.layer.2.output.dense.weight', 'text_encoder.encoder.layer.7.crossattention.self.query.bias', 'text_encoder.encoder.layer.1.crossattention.self.value.bias', 'text_encoder.encoder.layer.8.attention.output.dense.weight', 'text_encoder.encoder.layer.8.output.dense.weight', 'text_encoder.encoder.layer.4.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.crossattention.output.dense.bias', 'text_encoder.encoder.layer.1.intermediate.dense.weight', 'text_encoder.encoder.layer.1.crossattention.self.key.weight', 'text_encoder.encoder.layer.5.attention.self.query.weight', 'text_encoder.encoder.layer.10.attention.self.value.bias', 'text_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.crossattention.output.dense.bias', 'text_encoder.encoder.layer.7.crossattention.output.dense.bias', 'text_encoder.encoder.layer.5.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.output.LayerNorm.weight', 'itm_head.weight', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.crossattention.self.key.bias', 'text_encoder.encoder.layer.10.output.LayerNorm.bias', 'vision_proj.weight', 'text_encoder.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.intermediate.dense.bias', 'text_encoder.encoder.layer.11.crossattention.self.value.bias', 'text_encoder.encoder.layer.8.attention.self.query.weight', 'text_encoder.encoder.layer.4.crossattention.self.value.bias', 'text_encoder.encoder.layer.9.attention.self.value.weight', 'text_encoder.encoder.layer.3.output.dense.bias', 'text_encoder.encoder.layer.9.attention.output.dense.weight', 'text_encoder.encoder.layer.0.attention.output.dense.weight', 'text_encoder.encoder.layer.5.crossattention.self.key.weight', 'text_encoder.encoder.layer.6.attention.output.dense.bias', 'text_encoder.encoder.layer.8.crossattention.self.key.bias', 'text_encoder.encoder.layer.6.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.attention.self.key.bias', 'text_encoder.encoder.layer.10.attention.self.query.weight', 'text_encoder.encoder.layer.6.crossattention.self.key.bias', 'text_encoder.encoder.layer.2.crossattention.self.value.weight', 'text_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.intermediate.dense.weight', 'text_encoder.encoder.layer.0.crossattention.output.dense.bias', 'text_encoder.encoder.layer.7.attention.output.dense.bias', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.crossattention.self.key.bias', 'text_encoder.encoder.layer.11.attention.self.key.weight', 'text_encoder.encoder.layer.10.attention.output.dense.bias', 'text_encoder.encoder.layer.2.attention.output.dense.weight', 'text_encoder.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.crossattention.self.query.bias', 'text_encoder.encoder.layer.5.crossattention.self.value.weight', 'text_encoder.encoder.layer.1.output.dense.weight', 'text_encoder.encoder.layer.7.attention.self.value.weight', 'text_encoder.encoder.layer.0.attention.self.value.bias', 'text_encoder.encoder.layer.6.attention.output.dense.weight', 'text_encoder.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.attention.self.key.bias', 'text_encoder.encoder.layer.7.attention.self.query.bias', 'text_encoder.encoder.layer.4.attention.output.dense.weight', 'text_encoder.encoder.layer.4.output.dense.weight', 'text_encoder.encoder.layer.4.attention.self.key.bias', 'text_encoder.encoder.layer.4.crossattention.self.value.weight', 'text_encoder.encoder.layer.0.crossattention.self.value.weight', 'text_encoder.encoder.layer.4.crossattention.self.query.weight', 'vision_proj.bias', 'text_encoder.encoder.layer.9.attention.self.query.weight', 'text_encoder.encoder.layer.9.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.crossattention.self.value.bias', 'text_encoder.encoder.layer.3.crossattention.output.dense.bias', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.attention.self.query.bias', 'text_encoder.encoder.layer.11.crossattention.output.dense.weight', 'text_encoder.encoder.layer.3.crossattention.output.dense.weight', 'text_encoder.encoder.layer.7.intermediate.dense.weight', 'text_encoder.encoder.layer.10.attention.self.key.bias', 'text_encoder.encoder.layer.0.attention.self.key.bias', 'text_encoder.encoder.layer.9.attention.self.value.bias', 'text_encoder.encoder.layer.7.intermediate.dense.bias', 'text_encoder.encoder.layer.1.attention.self.value.weight', 'text_encoder.encoder.layer.2.crossattention.self.query.bias', 'text_encoder.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.attention.self.key.bias', 'text_encoder.encoder.layer.5.attention.self.value.weight', 'text_encoder.encoder.layer.11.attention.self.value.bias', 'text_encoder.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.intermediate.dense.weight', 'text_encoder.encoder.layer.4.intermediate.dense.bias', 'text_encoder.encoder.layer.8.intermediate.dense.weight', 'text_encoder.encoder.layer.2.crossattention.self.query.weight', 'text_encoder.encoder.layer.11.attention.self.key.bias', 'text_encoder.encoder.layer.11.crossattention.self.key.weight', 'text_encoder.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.attention.self.value.bias', 'text_encoder.encoder.layer.8.attention.output.dense.bias', 'text_encoder.encoder.layer.10.output.dense.bias', 'text_encoder.encoder.layer.5.crossattention.output.dense.bias', 'text_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.crossattention.self.query.weight', 'text_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.attention.output.dense.bias', 'text_encoder.encoder.layer.0.crossattention.self.key.weight', 'text_encoder.encoder.layer.1.crossattention.output.dense.bias', 'text_encoder.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.attention.self.value.weight', 'text_encoder.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.crossattention.self.value.weight', 'text_encoder.encoder.layer.10.crossattention.self.query.bias', 'text_encoder.encoder.layer.1.attention.self.key.weight', 'text_encoder.encoder.layer.6.output.dense.weight', 'text_encoder.encoder.layer.6.crossattention.output.dense.weight', 'text_encoder.encoder.layer.10.output.dense.weight', 'text_encoder.encoder.layer.9.attention.self.query.bias', 'text_encoder.encoder.layer.3.output.dense.weight', 'text_encoder.encoder.layer.7.crossattention.self.query.weight', 'text_encoder.encoder.layer.4.crossattention.self.key.weight', 'text_encoder.encoder.layer.9.attention.self.key.weight', 'text_encoder.encoder.layer.3.intermediate.dense.weight', 'text_encoder.encoder.layer.3.crossattention.self.query.bias', 'text_encoder.encoder.layer.0.output.LayerNorm.weight', 'text_proj.bias', 'text_encoder.encoder.layer.8.attention.self.query.bias', 'text_encoder.encoder.layer.0.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.intermediate.dense.weight', 'text_encoder.encoder.layer.6.crossattention.self.query.bias', 'text_encoder.encoder.layer.11.attention.output.dense.weight', 'text_encoder.encoder.layer.1.attention.self.query.weight', 'text_encoder.encoder.layer.7.output.dense.bias', 'text_encoder.encoder.layer.5.attention.output.dense.bias', 'text_encoder.embeddings.word_embeddings.weight', 'text_encoder.encoder.layer.0.output.dense.bias', 'text_encoder.encoder.layer.8.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.crossattention.self.key.weight', 'text_encoder.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.attention.self.query.weight', 'text_encoder.encoder.layer.10.crossattention.self.key.weight', 'text_encoder.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.attention.self.value.weight', 'text_encoder.encoder.layer.1.crossattention.self.key.bias', 'text_encoder.encoder.layer.8.crossattention.self.query.weight', 'text_encoder.encoder.layer.1.attention.output.dense.weight', 'text_encoder.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.attention.self.key.weight', 'text_encoder.encoder.layer.6.crossattention.self.value.bias', 'text_encoder.encoder.layer.1.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.crossattention.self.key.bias', 'text_encoder.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.attention.self.query.bias', 'text_encoder.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.attention.output.dense.bias', 'text_encoder.encoder.layer.0.attention.self.query.bias', 'text_encoder.encoder.layer.4.crossattention.output.dense.weight', 'text_encoder.encoder.layer.2.attention.self.value.bias', 'text_encoder.encoder.layer.9.crossattention.self.query.weight', 'text_encoder.encoder.layer.9.crossattention.output.dense.bias', 'text_encoder.encoder.layer.9.output.dense.bias', 'text_encoder.encoder.layer.2.crossattention.self.key.weight', 'text_encoder.encoder.layer.4.attention.self.key.weight', 'text_encoder.encoder.layer.7.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.attention.self.value.bias', 'text_encoder.encoder.layer.10.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.attention.self.value.bias', 'text_encoder.encoder.layer.6.crossattention.self.value.weight', 'text_encoder.encoder.layer.9.crossattention.self.value.bias', 'text_encoder.encoder.layer.11.crossattention.self.key.bias', 'text_encoder.encoder.layer.5.attention.self.key.weight', 'text_encoder.encoder.layer.9.attention.output.dense.bias', 'text_encoder.encoder.layer.4.output.LayerNorm.bias', 'text_encoder.embeddings.LayerNorm.bias', 'text_encoder.encoder.layer.2.crossattention.output.dense.weight', 'text_encoder.encoder.layer.4.crossattention.output.dense.bias', 'text_encoder.encoder.layer.2.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.output.dense.weight']\n",
      "- This IS expected if you are initializing ConCapModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ConCapModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ConCapModel were not initialized from the model checkpoint at ./work_dir/pt_source_14m/ and are newly initialized: ['decoder_prefix.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlipConfig, BlipProcessor\n",
    "from model.concap_model import ConCapModel\n",
    "\n",
    "pt=\"./work_dir/pt_source_14m/\"\n",
    "\n",
    "model_conf=BlipConfig.from_pretrained(pt)\n",
    "# test_mo=BlipModel.from_pretrained(mod)\n",
    "# test_itc=ConCapModelNew(model_conf, config[\"text_model\"])\n",
    "test_itc=ConCapModel.from_pretrained(pt, config[\"text_model\"])\n",
    "preprocessor=BlipProcessor.from_pretrained(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b78c09cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------dataset loading-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4e293306064a41a3\n",
      "Found cached dataset json (/home/lab-gao.zhenpeng/.cache/huggingface/datasets/json/default-4e293306064a41a3/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load train split from /148Dataset/data-gao.zhenpeng/PCap/personality_captions/train.json\n",
      "Length: 186800\n",
      "-------dataset load done-------\n"
     ]
    }
   ],
   "source": [
    "import data\n",
    "\n",
    "batch=2\n",
    "test_ds=data.build_pcap_dataset(config, preprocessor, device, \"train\")\n",
    "test_dl=data.build_dataloader(test_ds,batch,0,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f90fbb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([2, 3, 224, 224])\n",
      "input_ids torch.Size([2, 90])\n",
      "attention_mask torch.Size([2, 90])\n",
      "prefix_ids torch.Size([2, 16])\n",
      "labels torch.Size([2, 90])\n"
     ]
    }
   ],
   "source": [
    "dl_item=dict(next(iter(test_dl)))\n",
    "dl_item[\"labels\"]=dl_item[\"input_ids\"]\n",
    "\n",
    "for k,item in dl_item.items():\n",
    "    print(k, item.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c8ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.utils.generation import generate_nucleus\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_test(model, pixel_values, preprocessor, topk=10, topp=0.9, temp=0.75):\n",
    "    device=model.device\n",
    "    batch_size = pixel_values.shape[0]\n",
    "    vision_outputs = model.vision_model(\n",
    "        pixel_values=pixel_values\n",
    "    )\n",
    "\n",
    "    image_embeds=vision_outputs[0]\n",
    "    image_attention_mask = torch.ones(image_embeds.size()[:-1], \n",
    "                                        dtype=torch.long).to(device)\n",
    "    \n",
    "    # dummy pfx\n",
    "    prefix_embs=torch.ones(batch_size, 1, model.config.text_config.hidden_size).to(device)\n",
    "    prefix_embs.fill_(model.config.text_config.pad_token_id)\n",
    "    # dummy pfx id\n",
    "    prefix_ids=preprocessor.tokenizer([\"a picture of\"])[\"input_ids\"][0][1:-1]\n",
    "    prefix_ids=torch.tensor(prefix_ids, dtype=torch.long, device=device).repeat(batch,1)\n",
    "    results=model.generate(pixel_values, prefix_ids, num_beams=3, max_length=30)\n",
    "    results=preprocessor.batch_decode(results)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53e08ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1037, 6302, 1997]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.tokenizer([\"a photo of\"])[\"input_ids\"][0][1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c700f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_test(test_itc, dl_item[\"pixel_values\"], preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d60e7733",
   "metadata": {},
   "outputs": [],
   "source": [
    "out=test_itc(**dl_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27138103",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb19124",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen=test_itc.generate(pixel_values=dl_item[\"pixel_values\"],\n",
    "                      prefix_ids=dl_item[\"prefix_ids\"],\n",
    "                      max_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c938d803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b490b407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 30524])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"dec_logits\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faebc2b",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91445342",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139665905517808\n",
      "139665905517808\n"
     ]
    }
   ],
   "source": [
    "print(id(test_itc.text_encoder.embeddings.word_embeddings.weight))\n",
    "print(id(test_itc.text_decoder.bert.embeddings.word_embeddings.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8275a8b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------dataset loading-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4e293306064a41a3\n",
      "Found cached dataset json (/home/lab-gao.zhenpeng/.cache/huggingface/datasets/json/default-4e293306064a41a3/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load pretrain split from /148Dataset/data-gao.zhenpeng/PCap/personality_captions/train.json\n",
      "Length: 186800\n",
      "-------dataset load done-------\n"
     ]
    }
   ],
   "source": [
    "import data\n",
    "\n",
    "batch=2\n",
    "test_ds=data.build_pcap_dataset(config, preprocessor, device, \"pretrain\")\n",
    "test_dl=data.build_dataloader(test_ds,batch,0,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a798969",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def merge_negative(entity, device):\n",
    "    pixel_values=entity.pop(\"pixel_values\").to(device)\n",
    "    bsz=pixel_values.shape[0]\n",
    "    merges={}\n",
    "    # add label first\n",
    "    \n",
    "    for merge_keys in [\"input_ids\", \"attention_mask\", \"prefix_ids\"]:\n",
    "        gnd_truth=entity.pop(merge_keys)\n",
    "        fake=entity.pop(\"neg_{}\".format(merge_keys))\n",
    "        merges[merge_keys]=torch.cat([gnd_truth, fake], dim=0).to(device)\n",
    "    \n",
    "    labels=merges[\"input_ids\"].clone()[0:bsz, :]\n",
    "    \n",
    "    return dict(pixel_values=pixel_values, labels=labels, **merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adfd87b5",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([2, 3, 224, 224])\n",
      "labels torch.Size([2, 90])\n",
      "input_ids torch.Size([4, 90])\n",
      "attention_mask torch.Size([4, 90])\n",
      "prefix_ids torch.Size([4, 16])\n"
     ]
    }
   ],
   "source": [
    "dl_item=next(iter(test_dl))\n",
    "dl_merged=merge_negative(dl_item, device)\n",
    "\n",
    "for k,item in dl_merged.items():\n",
    "    print(k, item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b8b2a9f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ViT_prefix': True, 'Text_dec_prefix': False}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_itc.prefix_decoder_(False)\n",
    "test_itc.get_pfx_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d45d11b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_itc.shrink_sv=True\n",
    "out=test_itc(**dl_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0b92d0f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_itc': tensor(1.1126, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " 'loss_lm': tensor(11.1471, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " 'img_embeds': tensor([[[ 0.3283,  0.0837,  0.2164,  ...,  0.3975, -0.0954,  0.1929],\n",
       "          [-0.2062,  1.9501,  0.8359,  ..., -0.4714,  0.0673,  0.6110],\n",
       "          [ 0.7568,  1.3493,  0.0706,  ..., -0.1209, -0.4022,  0.3815],\n",
       "          ...,\n",
       "          [-0.3864,  0.7855,  0.2927,  ..., -0.1600, -0.0219,  0.3021],\n",
       "          [ 0.3155,  0.6456,  0.1878,  ..., -0.0556,  0.1236,  0.0337],\n",
       "          [ 0.0796,  0.8557, -0.3193,  ..., -0.7421, -0.2081,  0.5640]],\n",
       " \n",
       "         [[-0.4144, -0.1031, -0.0307,  ..., -0.2658,  0.1297,  0.4910],\n",
       "          [ 0.2670,  0.0143, -1.7533,  ...,  0.3614, -0.0510, -0.8377],\n",
       "          [-1.6325,  1.1176,  0.2287,  ...,  0.1843, -0.4375, -0.0537],\n",
       "          ...,\n",
       "          [ 0.1410,  0.4060, -0.7311,  ..., -0.6398,  1.2749, -0.1180],\n",
       "          [-0.2414,  0.5608,  0.6896,  ..., -0.5657,  0.3225,  0.0545],\n",
       "          [ 0.2522,  0.9460, -2.3246,  ..., -0.3742,  0.2996, -0.4843]]],\n",
       "        device='cuda:0'),\n",
       " 'dec_logits': tensor([[[-15.0563, -14.9517, -14.9780,  ..., -14.5845, -13.8695, -13.9099],\n",
       "          [-16.3801, -16.4325, -16.2732,  ..., -16.5995, -15.6000, -15.2892],\n",
       "          [-15.9155, -15.9398, -15.7752,  ..., -15.9140, -15.0719, -14.7100],\n",
       "          ...,\n",
       "          [-16.4878, -16.5512, -16.3745,  ..., -16.6180, -15.7123, -15.3432],\n",
       "          [-16.4416, -16.5063, -16.3281,  ..., -16.5806, -15.6675, -15.3012],\n",
       "          [-16.4556, -16.5196, -16.3414,  ..., -16.5924, -15.6926, -15.3251]],\n",
       " \n",
       "         [[-16.2750, -16.2718, -16.2108,  ..., -15.8625, -16.2542, -16.1743],\n",
       "          [-16.8850, -17.0198, -16.7911,  ..., -15.7992, -17.1283, -16.2447],\n",
       "          [-16.7778, -16.8774, -16.5573,  ..., -15.8267, -16.9359, -16.1092],\n",
       "          ...,\n",
       "          [-17.2926, -17.4690, -17.1696,  ..., -16.4126, -17.6476, -16.6511],\n",
       "          [-17.3132, -17.4857, -17.1906,  ..., -16.4248, -17.6697, -16.6546],\n",
       "          [-17.3283, -17.4997, -17.2032,  ..., -16.4484, -17.6795, -16.6775]]],\n",
       "        device='cuda:0', grad_fn=<ViewBackward0>)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b63436b3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen=test_itc.generate(\n",
    "    pixel_values=dl_merged[\"pixel_values\"],\n",
    "    prefix_ids=dl_merged[\"prefix_ids\"][batch:],\n",
    "    max_length=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae8dee2f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a red red red red red red red red red red red a red red red red red red red red red red red red red red red red',\n",
       " 'black black black black black black a a a a a a a a black a black a black a black a black a black a black a black']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.batch_decode(gen, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3be67",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22907dc1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def img_hash_to_addr(x, img_addr, img_attr):\n",
    "    x[\"images\"]=osp.join(img_addr, x.pop(\"image_hash\")+img_attr)\n",
    "\n",
    "    return x\n",
    "\n",
    "img_addr=config[\"dataset\"][\"img_path\"]\n",
    "img_attr=config[\"dataset\"][\"img_attr\"]\n",
    "\n",
    "with open(\"./misc/inference_src.json\") as f:\n",
    "    inference_list=json.load(f)\n",
    "\n",
    "inference_list=[img_hash_to_addr(x, img_addr, img_attr) for x in inference_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a295feb0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load ckpt\n",
    "epoch=8\n",
    "inference_ckpt=\"./work_dir/pretrain/pretrain_epoch{}.pth\".format(epoch)\n",
    "state_dict=torch.load(inference_ckpt)\n",
    "missing_keys, unexp_keys=model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34255571",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ViT_prefix': True, 'Text_dec_prefix': False}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_pfx_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b212a08a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Inference start-------\n",
      "------Inference end-------\n"
     ]
    }
   ],
   "source": [
    "inf_r=evaluate_model.inference(model, config, processor, device, inference_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36011c85",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'personality': 'Stupid',\n",
       "  'images': '/148Dataset/data-gao.zhenpeng/PCap/yfcc_images/15cdd44cf6d73b8f0b64352372a91c1.jpg',\n",
       "  'comment_model': 'i would love to take this car for a spin, it would be so much fun',\n",
       "  'comment_gndtruth': 'why do cones even exist'},\n",
       " {'personality': 'Opinionated',\n",
       "  'images': '/148Dataset/data-gao.zhenpeng/PCap/yfcc_images/2ed2094a52eb3579f4ef7c5e501db5.jpg',\n",
       "  'comment_model': 'i love the way her dress matches the wall',\n",
       "  'comment_gndtruth': 'I would never wear that top with those pants.'},\n",
       " {'personality': 'Deep',\n",
       "  'images': '/148Dataset/data-gao.zhenpeng/PCap/yfcc_images/f07589c35e378e3043fd5f642513bb64.jpg',\n",
       "  'comment_model': \"i'm not even sure what kind of bug this is\",\n",
       "  'comment_gndtruth': 'This was found in a glacier.'},\n",
       " {'personality': 'Mystical',\n",
       "  'images': '/148Dataset/data-gao.zhenpeng/PCap/yfcc_images/ac75942b5d6261354442cf502a2bb7dd.jpg',\n",
       "  'comment_model': 'i would love to see this in real life, but i have to look at a bunch of these pictures and teach myself how to take better pictures',\n",
       "  'comment_gndtruth': \"Cloudy ward evenings in natures' golden hue.\"},\n",
       " {'personality': 'Grim',\n",
       "  'images': '/148Dataset/data-gao.zhenpeng/PCap/yfcc_images/145c579124825a6fb82c5eee26d7a3ab.jpg',\n",
       "  'comment_model': \"i'm going to put my name all over this drawer\",\n",
       "  'comment_gndtruth': 'You could use any of these to strangle someone.'},\n",
       " {'personality': 'Calm',\n",
       "  'images': '/148Dataset/data-gao.zhenpeng/PCap/yfcc_images/d9a74f6bc2d3cb0a517f1c6bd6dfdeb.jpg',\n",
       "  'comment_model': 'i love this place, i hope i can live here forever',\n",
       "  'comment_gndtruth': 'Cool picture man.'},\n",
       " {'personality': 'Lazy',\n",
       "  'images': '/148Dataset/data-gao.zhenpeng/PCap/yfcc_images/837325523e1bd313536c18e63f3c252.jpg',\n",
       "  'comment_model': \"i'm not sure if this was taken in the us, or a used photo\",\n",
       "  'comment_gndtruth': 'I could sit under that tree all day and not do a thjng'},\n",
       " {'personality': 'Creative',\n",
       "  'images': '/148Dataset/data-gao.zhenpeng/PCap/yfcc_images/f67bbbd0ffe146e76caad64022548aba.jpg',\n",
       "  'comment_model': \"i'm not sure what they're doing here, but i'm sure they're doing a great job\",\n",
       "  'comment_gndtruth': 'Just a little higher there and a little more base and it will create the sound'},\n",
       " {'personality': 'Absentminded',\n",
       "  'images': '/148Dataset/data-gao.zhenpeng/PCap/yfcc_images/d039ee869ab8618bba1b78eb2e44bf3.jpg',\n",
       "  'comment_model': 'this is a very articulate piece of art',\n",
       "  'comment_gndtruth': 'I would jump through this.'},\n",
       " {'personality': 'Nihilistic',\n",
       "  'images': '/148Dataset/data-gao.zhenpeng/PCap/yfcc_images/13d6c33b85da7749aa62d966cc91d7.jpg',\n",
       "  'comment_model': 'i wish i had a pair of similar shoes',\n",
       "  'comment_gndtruth': 'THIS BOY LOOKS THE SAME IN BOTH PICTURES BUT IS A TERRIBLE SERIAL KILLER.'},\n",
       " {'personality': 'Destructive',\n",
       "  'images': '/148Dataset/data-gao.zhenpeng/PCap/yfcc_images/17bb5c2fddbd6ffcd4d35d43755cadd.jpg',\n",
       "  'comment_model': 'i bet he missed the ball',\n",
       "  'comment_gndtruth': \"I'm no pro, but he is definitely holding that bat wrong.\"},\n",
       " {'personality': 'Practical',\n",
       "  'images': '/148Dataset/data-gao.zhenpeng/PCap/yfcc_images/17bb5c2fddbd6ffcd4d35d43755cadd.jpg',\n",
       "  'comment_model': 'i bet he missed the ball',\n",
       "  'comment_gndtruth': 'no/idea'},\n",
       " {'personality': 'Assertive',\n",
       "  'images': '/148Dataset/data-gao.zhenpeng/PCap/yfcc_images/f08b5d8261d8363bba5ca5b0843037bf.jpg',\n",
       "  'comment_model': \"i'm so excited to see the finished product\",\n",
       "  'comment_gndtruth': 'consider this place owned by the state '},\n",
       " {'personality': 'Practical',\n",
       "  'images': '/148Dataset/data-gao.zhenpeng/PCap/yfcc_images/6fc987ebf4fdea174621c361bd827fc6.jpg',\n",
       "  'comment_model': 'i need this now',\n",
       "  'comment_gndtruth': 'I totally could go without the Nutella.'},\n",
       " {'personality': 'Aggressive',\n",
       "  'images': '/148Dataset/data-gao.zhenpeng/PCap/yfcc_images/594e9b352cf52b193dd824c9caa16a7.jpg',\n",
       "  'comment_model': 'i bet they are trying to solve the problems that they are working hard to get here',\n",
       "  'comment_gndtruth': 'I bet they are discussing about pointless stuff. I HATE these people!'},\n",
       " {'personality': 'Cold',\n",
       "  'images': '/148Dataset/data-gao.zhenpeng/PCap/yfcc_images/f2ed11c64f4cda6fd3fa35487f99d412.jpg',\n",
       "  'comment_model': \"i'm not sure what to think of this\",\n",
       "  'comment_gndtruth': \"Those chairs look uncomfortable. You couldn't pay me to sit in them.\"},\n",
       " {'personality': 'Exciting',\n",
       "  'images': '/148Dataset/data-gao.zhenpeng/PCap/yfcc_images/dfcc3c611e334a37e8b2f3475b4a946.jpg',\n",
       "  'comment_model': 'this painting is so beautiful it reminds me of a beautiful mind',\n",
       "  'comment_gndtruth': 'My painting one first prize'},\n",
       " {'personality': 'Frivolous (Trivial, Silly)',\n",
       "  'images': '/148Dataset/data-gao.zhenpeng/PCap/yfcc_images/2641f7424531f8bdbf241bc9c9a631f.jpg',\n",
       "  'comment_model': 'i love the way the cars all come together like that',\n",
       "  'comment_gndtruth': 'WHERE YOU CAN GO DOWNTOWN'},\n",
       " {'personality': 'Open',\n",
       "  'images': '/148Dataset/data-gao.zhenpeng/PCap/yfcc_images/1ceb484e59e9fa56971780d0f87ceb.jpg',\n",
       "  'comment_model': 'i bet she sings so much',\n",
       "  'comment_gndtruth': 'open the heart to share with others'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd75af41",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## test_automated_LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22932656",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lm_json=\"./work_dir/pretrain/lm_230806b.json\"\n",
    "lm_results=load_cached_result(lm_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74dd6180",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, './work_dir/pretrain/pretrain_epoch2.pth'),\n",
       " (4, './work_dir/pretrain/pretrain_epoch4.pth'),\n",
       " (6, './work_dir/pretrain/pretrain_epoch6.pth'),\n",
       " (8, './work_dir/pretrain/pretrain_epoch8.pth'),\n",
       " (10, './work_dir/pretrain/pretrain_epoch10.pth')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tested_epoches=[x[\"epoch\"] for x in lm_results]\n",
    "ckpt_dir=\"./work_dir/pretrain/\"\n",
    "fname_pattern=re.compile(r\"^pretrain_epoch(\\d+?)\\.pth$\")\n",
    "ckpt_lists=find_ckpt_files(ckpt_dir, fname_pattern)\n",
    "un_ckpt_lists=list(filter(lambda x:x[0] not in tested_epoches, ckpt_lists))\n",
    "\n",
    "un_ckpt_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f2d6d8f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating epoch 2\n",
      "\n",
      "------Enter evaluation------\n",
      "Using pycocoeval metric currently. \n",
      "\n",
      "-------dataset loading-------\n",
      "\n",
      "Load test split from /148Dataset/data-gao.zhenpeng/PCap/personality_captions/test.json\n",
      "Length: 9996\n",
      "-------dataset load done-------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507130701329430493f7da9ff982544b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test set:   0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 130267 tokens at 902555.02 tokens per second.\n",
      "Aug 07, 2023 1:57:41 AM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: ? (U+D83E, decimal: 55358)\n",
      "PTBTokenizer tokenized 529941 tokens at 1826223.87 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ['BLEU_1', 'BLEU_4']...\n",
      "{'testlen': 120272, 'reflen': 109450, 'guess': [120272, 110276, 100280, 90284], 'correct': [36957, 4330, 598, 158]}\n",
      "ratio: 1.0988761991776967\n",
      "Evaluating ROUGE_L...\n",
      "Evaluating CIDEr...\n",
      "Evaluating SPICE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 5.120 s\n",
      "\n",
      "------Finished evaluation------\n",
      "{'BLEU_1': 30.728, 'BLEU_4': 1.884, 'ROUGE_L': 21.475, 'CIDEr': 8.056, 'SPICE': 4.49}\n",
      "\n",
      "Evaluating epoch 4\n",
      "\n",
      "------Enter evaluation------\n",
      "Using pycocoeval metric currently. \n",
      "\n",
      "-------dataset loading-------\n",
      "\n",
      "Load test split from /148Dataset/data-gao.zhenpeng/PCap/personality_captions/test.json\n",
      "Length: 9996\n",
      "-------dataset load done-------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c13d5265904bbcb818bb42600d6871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test set:   0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 100169 tokens at 638429.85 tokens per second.\n",
      "Aug 07, 2023 2:03:31 AM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: ? (U+D83E, decimal: 55358)\n",
      "PTBTokenizer tokenized 529941 tokens at 1941192.70 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ['BLEU_1', 'BLEU_4']...\n",
      "{'testlen': 90174, 'reflen': 86286, 'guess': [90174, 80178, 70182, 60186], 'correct': [28663, 3928, 835, 284]}\n",
      "ratio: 1.0450594534455062\n",
      "Evaluating ROUGE_L...\n",
      "Evaluating CIDEr...\n",
      "Evaluating SPICE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 4.818 s\n",
      "\n",
      "------Finished evaluation------\n",
      "{'BLEU_1': 31.786, 'BLEU_4': 3.058, 'ROUGE_L': 20.954, 'CIDEr': 8.435, 'SPICE': 2.691}\n",
      "\n",
      "Evaluating epoch 6\n",
      "\n",
      "------Enter evaluation------\n",
      "Using pycocoeval metric currently. \n",
      "\n",
      "-------dataset loading-------\n",
      "\n",
      "Load test split from /148Dataset/data-gao.zhenpeng/PCap/personality_captions/test.json\n",
      "Length: 9996\n",
      "-------dataset load done-------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94839466e0ba44bfa3c3fd544e895712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test set:   0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 103782 tokens at 840992.29 tokens per second.\n",
      "Aug 07, 2023 2:08:16 AM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: ? (U+D83E, decimal: 55358)\n",
      "PTBTokenizer tokenized 529941 tokens at 1496498.04 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ['BLEU_1', 'BLEU_4']...\n",
      "{'testlen': 93787, 'reflen': 88474, 'guess': [93787, 83791, 73795, 63799], 'correct': [34294, 4731, 989, 321]}\n",
      "ratio: 1.060051540565578\n",
      "Evaluating ROUGE_L...\n",
      "Evaluating CIDEr...\n",
      "Evaluating SPICE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.3 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Threads( StanfordCoreNLP ) [21.570 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 32.11 s\n",
      "\n",
      "------Finished evaluation------\n",
      "{'BLEU_1': 36.566, 'BLEU_4': 3.435, 'ROUGE_L': 23.107, 'CIDEr': 9.34, 'SPICE': 3.747}\n",
      "\n",
      "Evaluating epoch 8\n",
      "\n",
      "------Enter evaluation------\n",
      "Using pycocoeval metric currently. \n",
      "\n",
      "-------dataset loading-------\n",
      "\n",
      "Load test split from /148Dataset/data-gao.zhenpeng/PCap/personality_captions/test.json\n",
      "Length: 9996\n",
      "-------dataset load done-------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faee80f818254098b523769b1fe4bcb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test set:   0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 110217 tokens at 877197.15 tokens per second.\n",
      "Aug 07, 2023 2:14:07 AM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: ? (U+D83E, decimal: 55358)\n",
      "PTBTokenizer tokenized 529941 tokens at 1515036.67 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ['BLEU_1', 'BLEU_4']...\n",
      "{'testlen': 100222, 'reflen': 92169, 'guess': [100222, 90226, 80230, 70234], 'correct': [33229, 4237, 833, 192]}\n",
      "ratio: 1.087372109928489\n",
      "Evaluating ROUGE_L...\n",
      "Evaluating CIDEr...\n",
      "Evaluating SPICE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.3 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.9 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Threads( StanfordCoreNLP ) [28.309 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 37.60 s\n",
      "\n",
      "------Finished evaluation------\n",
      "{'BLEU_1': 33.155, 'BLEU_4': 2.578, 'ROUGE_L': 21.557, 'CIDEr': 8.221, 'SPICE': 3.724}\n",
      "\n",
      "Evaluating epoch 10\n",
      "\n",
      "------Enter evaluation------\n",
      "Using pycocoeval metric currently. \n",
      "\n",
      "-------dataset loading-------\n",
      "\n",
      "Load test split from /148Dataset/data-gao.zhenpeng/PCap/personality_captions/test.json\n",
      "Length: 9996\n",
      "-------dataset load done-------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c83b33f1b1e40a3b1564c1c86c37577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test set:   0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 112893 tokens at 678552.72 tokens per second.\n",
      "Aug 07, 2023 2:20:12 AM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: ? (U+D83D, decimal: 55357)\n",
      "PTBTokenizer tokenized 529941 tokens at 1919012.10 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ['BLEU_1', 'BLEU_4']...\n",
      "{'testlen': 102898, 'reflen': 94573, 'guess': [102898, 92902, 82906, 72910], 'correct': [33232, 4193, 800, 211]}\n",
      "ratio: 1.0880272382180844\n",
      "Evaluating ROUGE_L...\n",
      "Evaluating CIDEr...\n",
      "Evaluating SPICE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.3 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.9 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [23.894 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 32.83 s\n",
      "\n",
      "------Finished evaluation------\n",
      "{'BLEU_1': 32.296, 'BLEU_4': 2.526, 'ROUGE_L': 21.247, 'CIDEr': 7.967, 'SPICE': 3.784}\n"
     ]
    }
   ],
   "source": [
    "for epoch, fname in un_ckpt_lists:\n",
    "    print(\"\\nEvaluating epoch {}\".format(epoch))\n",
    "\n",
    "    state_dict=torch.load(fname)\n",
    "    missing_keys, unexp_keys=model.load_state_dict(state_dict, strict=False)\n",
    "    \n",
    "    lm_result=evaluate_model.evaluate_lm(model, config, processor, device)\n",
    "    print(lm_result)\n",
    "    lm_results.append({\"epoch\": epoch, **lm_result})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a460ded",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 2,\n",
       "  'BLEU_1': 30.728,\n",
       "  'BLEU_4': 1.884,\n",
       "  'ROUGE_L': 21.475,\n",
       "  'CIDEr': 8.056,\n",
       "  'SPICE': 4.49},\n",
       " {'epoch': 4,\n",
       "  'BLEU_1': 31.786,\n",
       "  'BLEU_4': 3.058,\n",
       "  'ROUGE_L': 20.954,\n",
       "  'CIDEr': 8.435,\n",
       "  'SPICE': 2.691},\n",
       " {'epoch': 6,\n",
       "  'BLEU_1': 36.566,\n",
       "  'BLEU_4': 3.435,\n",
       "  'ROUGE_L': 23.107,\n",
       "  'CIDEr': 9.34,\n",
       "  'SPICE': 3.747},\n",
       " {'epoch': 8,\n",
       "  'BLEU_1': 33.155,\n",
       "  'BLEU_4': 2.578,\n",
       "  'ROUGE_L': 21.557,\n",
       "  'CIDEr': 8.221,\n",
       "  'SPICE': 3.724},\n",
       " {'epoch': 10,\n",
       "  'BLEU_1': 32.296,\n",
       "  'BLEU_4': 2.526,\n",
       "  'ROUGE_L': 21.247,\n",
       "  'CIDEr': 7.967,\n",
       "  'SPICE': 3.784}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cc3ba18",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dump_result(lm_results, lm_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b719d510",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## test ITM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcc19b37",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------dataset loading-------\n",
      "\n",
      "Load val split from /148Dataset/data-gao.zhenpeng/PCap/personality_captions/val.json\n",
      "Length: 4999\n",
      "-------dataset load done-------\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "from data import build_dataloader, build_pcap_dataset\n",
    "eval_batch=16\n",
    "eval_workers=4\n",
    "eval_ds=build_pcap_dataset(config, processor, device, \"val\")\n",
    "eval_dl=build_dataloader(eval_ds, batch=eval_batch, num_workers=eval_workers, dist_training=False)\n",
    "\n",
    "total_steps=ceil(len(eval_ds)/eval_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "488676b1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "itm_json=\"./work_dir/pretrain/itm_230806.json\"\n",
    "itm_results=load_cached_result(itm_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04da354b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, './work_dir/pretrain/pretrain_epoch1.pth'),\n",
       " (2, './work_dir/pretrain/pretrain_epoch2.pth'),\n",
       " (3, './work_dir/pretrain/pretrain_epoch3.pth'),\n",
       " (4, './work_dir/pretrain/pretrain_epoch4.pth'),\n",
       " (5, './work_dir/pretrain/pretrain_epoch5.pth'),\n",
       " (6, './work_dir/pretrain/pretrain_epoch6.pth')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tested_epoches=[x[\"epoch\"] for x in itm_results]\n",
    "\n",
    "ckpt_dir=\"./work_dir/pretrain/\"\n",
    "fname_pattern=re.compile(r\"^pretrain_epoch(\\d+?)\\.pth$\")\n",
    "ckpt_lists=find_ckpt_files(ckpt_dir, fname_pattern)\n",
    "un_ckpt_lists=list(filter(lambda x:x[0] not in tested_epoches, ckpt_lists))\n",
    "\n",
    "un_ckpt_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "970505d4",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating epoch 1 of ITM\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75a48cf35e84f23890f5c45b0f83165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "ITM loss: 0.8491393327713013\n",
      "ITM accuracy: 73.17%\n",
      "{'epoch': 1, 'accuracy': 73.1746, 'itm_loss': 0.8491}\n",
      "\n",
      "Evaluating epoch 2 of ITM\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009a6919f742410087d38a2487b52abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "ITM loss: 0.8590194582939148\n",
      "ITM accuracy: 72.83%\n",
      "{'epoch': 2, 'accuracy': 72.8346, 'itm_loss': 0.859}\n",
      "\n",
      "Evaluating epoch 3 of ITM\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5fe85c10a54a9a93b2cba6614cbd05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "ITM loss: 0.8761993646621704\n",
      "ITM accuracy: 55.81%\n",
      "{'epoch': 3, 'accuracy': 55.8112, 'itm_loss': 0.8762}\n",
      "\n",
      "Evaluating epoch 4 of ITM\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e8fa166c65b46328632fbe760014f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "ITM loss: 0.9726016521453857\n",
      "ITM accuracy: 35.55%\n",
      "{'epoch': 4, 'accuracy': 35.5471, 'itm_loss': 0.9726}\n",
      "\n",
      "Evaluating epoch 5 of ITM\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df0f5acefb243bab23a0c3c66aad3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "ITM loss: 1.1176306009292603\n",
      "ITM accuracy: 37.69%\n",
      "{'epoch': 5, 'accuracy': 37.6875, 'itm_loss': 1.1176}\n",
      "\n",
      "Evaluating epoch 6 of ITM\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46acbc78c4464a3fa28421ae2373542b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "ITM loss: 1.2276833057403564\n",
      "ITM accuracy: 39.29%\n",
      "{'epoch': 6, 'accuracy': 39.2879, 'itm_loss': 1.2277}\n"
     ]
    }
   ],
   "source": [
    "for epoch, fname in un_ckpt_lists:\n",
    "    print(\"\\nEvaluating epoch {} of ITM\".format(epoch))\n",
    "\n",
    "    state_dict=torch.load(fname)\n",
    "    missing_keys, unexp_keys=model.load_state_dict(state_dict, strict=False)\n",
    "    \n",
    "    itm_result=evaluate_model.evaluate_itm(model, eval_dl, device=device, \n",
    "                                           total_steps=total_steps, epoch=epoch)\n",
    "    print(itm_result[1])\n",
    "    itm_results.append(itm_result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3e994db",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1, 'accuracy': 73.1746, 'itm_loss': 0.8491},\n",
       " {'epoch': 2, 'accuracy': 72.8346, 'itm_loss': 0.859},\n",
       " {'epoch': 3, 'accuracy': 55.8112, 'itm_loss': 0.8762},\n",
       " {'epoch': 4, 'accuracy': 35.5471, 'itm_loss': 0.9726},\n",
       " {'epoch': 5, 'accuracy': 37.6875, 'itm_loss': 1.1176},\n",
       " {'epoch': 6, 'accuracy': 39.2879, 'itm_loss': 1.2277}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "12d1afa3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dump_result(itm_results, itm_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59244751",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc6e33d5",
   "metadata": {},
   "source": [
    "## test finetune inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffe2f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path as osp\n",
    "\n",
    "with open(\"./conf/caption_pcap_224.json\") as f:\n",
    "    config=json.load(f)\n",
    "\n",
    "pt=osp.join(config[\"work_dir\"], \"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ca39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate_model\n",
    "evaluate_model.tqdm_module(True)\n",
    "from finetune_pfx_dec import load_model as load_finetune_model\n",
    "\n",
    "model, processor, load_info=load_finetune_model(config, pt, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08709391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ViT_prefix': False, 'Text_dec_prefix': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_pfx_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85095e60",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'missing_keys': [],\n",
       " 'unexpected_keys': [],\n",
       " 'mismatched_keys': [],\n",
       " 'error_msgs': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8084adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_hash_to_addr(x, img_addr, img_attr):\n",
    "    x[\"images\"]=osp.join(img_addr, x.pop(\"image_hash\")+img_attr)\n",
    "\n",
    "    return x\n",
    "\n",
    "img_addr=config[\"dataset\"][\"img_path\"]\n",
    "img_attr=config[\"dataset\"][\"img_attr\"]\n",
    "\n",
    "with open(\"./misc/inference_src.json\") as f:\n",
    "    inference_list=json.load(f)\n",
    "\n",
    "inference_list=[img_hash_to_addr(x, img_addr, img_attr) for x in inference_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fc0cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ckpt\n",
    "inference_ckpt=osp.join(config[\"work_dir\"], \"last.pth\")\n",
    "state_dict=torch.load(inference_ckpt)\n",
    "missing_keys, unexp_keys=model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d511a9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_keys, unexp_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dc44fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ViT_prefix': False, 'Text_dec_prefix': True}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_pfx_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b50b5899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Inference start-------\n",
      "------Inference end-------\n"
     ]
    }
   ],
   "source": [
    "inf_r=evaluate_model.inference(model, config, processor, device, inference_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b11979",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inf_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acd9bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0542df0a51ed6904088303bd20323b51f480e8be3d4cdfe0027c89da43039b49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
